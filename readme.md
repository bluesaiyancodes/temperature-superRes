# Temperature Image Super-Resolution (SRResUNet)

Super-resolve low-resolution temperature (thermal) images using a lightweight **SRResUNet** model implemented in PyTorch.  
The project includes:

- A Hydra-driven **train/test** pipeline (`main.py`)
- A dataset **augmentation** pipeline (`augment.py`)
- A **PyQt GUI** app for single/multi-image inference (`ruperRes_gui.py`)
- Reproducible outputs (timestamped) and optional Weights & Biases logging

## ğŸ§­ Repository Structure

```
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # your raw LR/HR sources (see augment.py)
â”‚   â””â”€â”€ test/         # can hold ad-hoc test images
â”œâ”€â”€ debug/
â”‚   â”œâ”€â”€ debug_dataset.ipynb
â”‚   â””â”€â”€ debug_test.ipynb
â”œâ”€â”€ outputs/          # hydra-run outputs, auto timestamped
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ dataloader.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ SRResUNet.py
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tempResEnv/       # local env folder
â”œâ”€â”€ .gitignore
â”œâ”€â”€ augment.py
â”œâ”€â”€ main.py           # Main file for training and testing model
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ ruperRes_gui.py   # PyQt GUI for inference
```


## âš™ï¸ Environment Setup


Make and activate an environment
```bash
python -m venv <env>
source <env>/bin/activate
```

Intall the project by running the following program 
```bash
pip install -e .
```


## ğŸ“ Data & Augmentation

### Expected final dataset layout (after augmentation)
`augment.py` builds an **augmented** dataset under `data/augmented/`:

```
data/augmented/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ low_res/       # 120 Ã— 160, RGB PNGs
â”‚   â””â”€â”€ high_res/      # 480 Ã— 640, RGB PNGs
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ low_res/
â”‚   â””â”€â”€ high_res/
â””â”€â”€ test/
    â”œâ”€â”€ low_res/
    â””â”€â”€ high_res/
```

- LR images are 120Ã—160; HR images are 480Ã—640 (4Ã— upscaling).
- Filenames **must match** across LR/HR (the dataloader pairs by name).

### Build the augmented dataset

`augment.py` expects your raw inputs in:

- `data/raw/webcam_frames7/` (LR source)
- `data/raw/flir_frames7/`   (HR source)

Run:

```bash
python augment.py
```

Key details from `augment.py`:

- Applies geometric + photometric transforms with **Albumentations**.
- Ensures HR is **480Ã—640** and LR is **120Ã—160**.
- Splits Train/Val/Test using the configured ratios and saves PNG pairs.


## ğŸ§ª Train & Test (Hydra)

### Config

Edit `configs/config.yaml` (example keys):

```yaml
dataset:
  name: custom
  path: data/augmented/
model:
  name: srresunet
  in_channels: 3
  out_channels: 3
  num_filters: 32
  num_residuals: 2
  upscale_factor: 4
training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.0001
other:
  wandb: false              # set to your project name to enable W&B
  run_testing: true
  run_testing_only: true    # set to false if training the model
  testing_only_model_path: outputs/2025-05-29/22-54-23/models/
```

Hydra creates a new timestamped directory under `outputs/` on each run and writes the resolved `config.yaml` there.

### Train

```bash
python main.py 
```

**What happens:**

- Datasets are loaded via `src/data/dataloader.py` (OpenCV â†’ RGB â†’ `[0,1]` float32, tensors shaped `(C,H,W)`).
- Model: `src/models/SRResUNet.py` with the configured channels/filters/residuals and `upscale_factor=4`.
- Training uses **L1 loss** and **Adam** optimizer.
- Best and final checkpoints saved to `outputs/<date>/<time>/models/`.

### Test (using best/final checkpoint)

**Option A â€” test only (no training), path given:**

```bash
python main.py \
  other.run_testing=true \
  other.run_testing_only=true \
  other.testing_only_model_path=outputs/2025-05-29/22-54-23/models/ \
  dataset.path=data/augmented
```

**Option B â€” after training in the same run:**

```bash
python main.py other.run_testing=true other.run_testing_only=false
```

**Outputs:**

- Per-image LR/GeneratedHR/GT saved under:
  `outputs/<date>/<time>/test_outputs/{input_lr,generated_hr,ground_truth_hr}/`
- Average test **L1 loss** printed and (if enabled) logged to W&B.


## ğŸ–¼ï¸ GUI Inference App (PyQt)

The GUI lets you super-resolve arbitrary images (not just the dataset pairs), with **padding/cropping** logic to avoid UNet size mismatches.

Launch:

```bash
python superRes_gui.py
```

**Workflow:**

1. **Load Model (.pth)** â€“ pick your `best_model.pth` or `final_model.pth`.
   - The app **auto-loads** the `config.yaml` next to the checkpoint if present.
2. **Open Image(s) / Directory** â€“ select one or more images (PNG/JPG/TIF...).
3. (Optional) **Choose Save Dir** and check â€œSave outputsâ€.
4. Click **Process Current** or **Process All**.
5. Navigate with **Previous / Next** (centered buttons), or **â†/â†’** keys.
6. The bottom bar shows a **0â€“100%** position slider (percentage) + per-run progress bar.

**Display:**

- Shows **Input** and **Output** previews with image sizes centered below each view.


## ğŸ“š References & Useful Links

- PyTorch: https://pytorch.org/  
- Hydra: https://hydra.cc/  
- Albumentations: https://albumentations.ai/  
- Weights & Biases: https://wandb.ai/  
- U-Net (Ronneberger et al., 2015): https://arxiv.org/abs/1505.04597  
- Super-Resolution Residual U-Net Model  (Chen et al., 2022): https://pubs.acs.org/doi/10.1021/acsomega.2c01435

> The SRResUNet in this repo  is motivated from the above papers for super-resolution; see the original papers above for background and design intuition.

